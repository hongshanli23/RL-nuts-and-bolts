\documentclass{beamer}
\usepackage[utf8]{inputenc}
\usepacakge{macros}

\title{An overview of reinforcement learning}
\author{Hongshan Li}
\begin{document}

\begin{frame}{We will cover}
\begin{itemize}
\item What problems RL solve
\item What assumptions RL put on the problems
\item Difference between supervised learning (SL) and RL
\item Ideas underneath classical as well deep RL algorithms
\end{itemize}
\end{frame}

\begin{frame}{Type of problems RL umbrella}
Sequential decision problems.

A trained RL agent needs to be able to make a 
sequence of "smart" decisions in an environment.

Environment is a general notion

Less abstractly: 
1. You (agent) are navigating through your career, your work, your coworkers, managers, customers (basically everything/one related to your job) is abstracted
to this notion of envrionment.

2. You  want to achieve your finanacial freedom, you need to make decisions
about investment. 

3. You have a crush on someone, and you need to make a sequence of smart actions to 
attract his/her attention

4. Playing amd winning Atari games

But what's metric for smartness?
\end{frame}

\begin{frame}{Notations / Assumptions}
RL boils down to the interaction between the \emph{agent} and \emph{env}

At time step $t$, 
\begin{itemize}
\item the state of the env is $S_t$; 
\item the agent takes an action $A_t$;
\item the env changes to the next state (because of $A_t$) to $S_{t+1}$
\item the env returns a reward $R_{t+1}$ to the agent
\end{itemize}

The only assumption RL has is that the transition from 
$S_t$ to $S_{t+1}$ does not depends on the history (i.e
$S_{t-1}, S_{t-2},\cdots$ do not matter)

Env obeys this assumption is called a Markov Decision Process (MDP)
\end{frame}

\begin{frame}{Bellman Optimality Equation}
How do you know your policy is optimal?
You can recursively define 
\begin{frame}{Value Iteration}
"Do the best you can"
Include algorithm
\end{frame}







