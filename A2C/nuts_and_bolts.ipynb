{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "structured-watershed",
   "metadata": {},
   "source": [
    "## Overview of A2C\n",
    "\n",
    "## Problems with on-policy gradients methods\n",
    "\n",
    "- high variance in training (compared to what?) \n",
    "    * compared to off-policy methods\n",
    "    \n",
    "- sample inefficiency (compared to what? )\n",
    "    * compared to off-policy methods\n",
    "    * a trajectory is sampled / used / then throw away\n",
    "    \n",
    "[Why policy gradient has high variance](https://www.quora.com/Why-does-the-policy-gradient-method-have-a-high-variance)\n",
    "\n",
    "\n",
    "## Problems with off-policy methods\n",
    "- bad samples are hoarded and resued\n",
    "    * on-policy policy gradient methods are leaner (minimalist life-style)\n",
    "    \n",
    "- old samples come from old policy\n",
    "    * to train the current policy, we need importance sampling to correct the distribution\n",
    "    * not easy to do\n",
    "    * so many off-policy methods don't do it\n",
    "\n",
    "We can forget about policy and learn a q-function, but then how do we handle \n",
    "problems with continuous action space?\n",
    "\n",
    "Conclusion: pick your poison. \n",
    "\n",
    "\n",
    "## A2C\n",
    "How A2C solve those common problems of on-policy policy gradient methods\n",
    "\n",
    "- High variance of policy gradient estimate\n",
    "    * parallel env to sample many trajectories \n",
    "- Sample inefficiency\n",
    "    * getting more samples\n",
    "\n",
    "## Additional benefits\n",
    "\n",
    "- More exploration of env\n",
    "- Easy to parallel\n",
    "\n",
    "## How to quantify stability? \n",
    "\n",
    "- Plot gradient, log probability of actions, advantages? \n",
    "    * really depends on the problem. For cartpole, it only takes a few steps to go from a good state to a bad state\n",
    "    * so just become log probabiliy varies a lot does not necessary mean the training is not stable\n",
    "    * For a finite episode problem like CartPole, the average reward for each epsisode seems to be a good measure of stability \n",
    "     \n",
    "But it is not that straightforward to compute moving average of episode rewards, because we have parallel envs: env 1 might end vhile env 2 is still running. If we have only 1 enviroment, then this programming task is reduced\n",
    "to keep tracking of moving average (which well-exercised leetcoder  should know how to do it in O(1) time and O(1) space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-error",
   "metadata": {},
   "source": [
    "## Stability\n",
    "\n",
    "1. Why do we want stability?\n",
    "* samples are generated from the policy\n",
    "* bad policy -> bad samples -> bad policy\n",
    "\n",
    "\n",
    "Factors affecting training stability\n",
    "* number of parallel environments\n",
    "* length of a trajectory\n",
    "* gradient \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-isolation",
   "metadata": {},
   "source": [
    "## Some debug work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-watch",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm ckpt/debug/*\n",
    "!rm log/debug/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "historic-sector",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-reservoir",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir log/debug --bind_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-chance",
   "metadata": {},
   "source": [
    "## Experiments: impact of number of parallel envs to stability\n",
    "Intuition is more parallel envs increases stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "discrete-optimization",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cartpole import a2c\n",
    "\n",
    "h = {\n",
    "    \"n_iters\": 1000,\n",
    "    #\"nenvs\": 16, # to be changed\n",
    "    \"ckpt_interval\": 100,\n",
    "    \"nsteps\" : 30, # length of trajectory\n",
    "    \"entropy_coef\" : 0.01, #1e-4, \n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"p_coef\": 1.0,\n",
    "    \"v_coef\":1.0,\n",
    "    \"gamma\": 0.99,\n",
    "    \"device\": \"cpu\",\n",
    "    \"clip_grad_norm\": False,\n",
    "    #\"ckpt_dir\": 'ckpt/debug', # to be changed \n",
    "    #\"log_dir\": 'log/debug' # to be changed\n",
    "}\n",
    "\n",
    "for nenvs in range(10, 16):\n",
    "    ckpt_dir = f'ckpt/nenv-{nenvs}'\n",
    "    log_dir = f'log/nenv-{nenvs}'\n",
    "    h[\"nenvs\"] = nenvs\n",
    "    h[\"ckpt_dir\"] = ckpt_dir\n",
    "    h[\"log_dir\"] = log_dir\n",
    "    a2c(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "european-demographic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def std(data, key):\n",
    "    \"\"\"compute std of data[key]\"\"\"\n",
    "    arr = [x[1] for x in data[key]]\n",
    "    return np.std(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "conscious-ordering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nenv: 10, std of moving average reward : 38.96572116541833\n",
      "nenv: 11, std of moving average reward : 43.87544792512868\n",
      "nenv: 12, std of moving average reward : 45.152174737019145\n",
      "nenv: 13, std of moving average reward : 42.73620335658786\n",
      "nenv: 14, std of moving average reward : 47.174675219639575\n",
      "nenv: 15, std of moving average reward : 45.26950135037092\n"
     ]
    }
   ],
   "source": [
    "# std of moving average for each experiment\n",
    "import numpy as np\n",
    "\n",
    "for nenv in range(10, 16):\n",
    "    log_dir = f'log/nenv-{nenv}'\n",
    "    logger = Logger(log_dir)\n",
    "    data = logger.load()\n",
    "    res = std(data, 'ma_rew')\n",
    "    print(f\"nenv: {nenv}, std of moving average reward : {res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "pleasant-timothy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nenv: 10, std of log probability of actions : 0.03559497094184473\n",
      "nenv: 11, std of log probability of actions : 0.03752836136908005\n",
      "nenv: 12, std of log probability of actions : 0.03567263716485237\n",
      "nenv: 13, std of log probability of actions : 0.03397460746003187\n",
      "nenv: 14, std of log probability of actions : 0.03964350383657634\n",
      "nenv: 15, std of log probability of actions : 0.03605159749922419\n"
     ]
    }
   ],
   "source": [
    "# std of log probability of actions\n",
    "\n",
    "for nenv in range(10, 16):\n",
    "    log_dir = f'log/nenv-{nenv}'\n",
    "    logger = Logger(log_dir)\n",
    "    data = logger.load()\n",
    "    res = std(data, 'log_pi_a')\n",
    "    print(f\"nenv: {nenv}, std of log probability of actions : {res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "turned-reputation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nenv: 10, std of loss: 126.65723240246216\n",
      "nenv: 11, std of loss: 126.60126408481901\n",
      "nenv: 12, std of loss: 123.85701479739846\n",
      "nenv: 13, std of loss: 126.65860444875733\n",
      "nenv: 14, std of loss: 117.70909187803315\n",
      "nenv: 15, std of loss: 125.34906857995779\n"
     ]
    }
   ],
   "source": [
    "# std of loss\n",
    "\n",
    "for nenv in range(10, 16):\n",
    "    log_dir = f'log/nenv-{nenv}'\n",
    "    logger = Logger(log_dir)\n",
    "    data = logger.load()\n",
    "    res = std(data, 'loss')\n",
    "    print(f\"nenv: {nenv}, std of loss: {res}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-provision",
   "metadata": {},
   "source": [
    "## N && B: a few techniques to make training is stable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-elements",
   "metadata": {},
   "source": [
    "## Clip gradients or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "primary-comment",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from cartpole import a2c\n",
    "\n",
    "h = {\n",
    "    \"n_iters\": 1000,\n",
    "    \"nenvs\": 16, # to be changed\n",
    "    \"ckpt_interval\": 10,\n",
    "    \"nsteps\" : 30, # length of trajectory\n",
    "    \"entropy_coef\" : 0.01, #1e-4, \n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"p_coef\": 1.0,\n",
    "    \"v_coef\":1.0,\n",
    "    \"gamma\": 0.99,\n",
    "    \"device\": \"cpu\",\n",
    "    #\"clip_grad_norm\": False,\n",
    "    #\"ckpt_dir\": 'ckpt/debug', # to be changed \n",
    "    #\"log_dir\": 'log/debug' # to be changed\n",
    "}\n",
    "\n",
    "for opt in [True, False]:\n",
    "    h['clip_grad_norm'] = opt\n",
    "    h['ckpt_dir'] = f'ckpt/clip-grad-{opt}'\n",
    "    h['log_dir'] = f'log/clip-grad-{opt}'\n",
    "    #a2c(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perfect-tongue",
   "metadata": {},
   "source": [
    "### Std of moving average reward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "another-montana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clip gradient: True, std of ma reward: 31.566684395435963\n",
      "clip gradient: False, std of ma reward: 46.7627088209243\n"
     ]
    }
   ],
   "source": [
    "for opt in [True, False]:\n",
    "    log_dir = f'log/clip-grad-{opt}'\n",
    "    logger = Logger(log_dir)\n",
    "    data = logger.load()\n",
    "    res = std(data, 'ma_rew')\n",
    "    print(f\"clip gradient: {opt}, std of ma reward: {res}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-approach",
   "metadata": {},
   "source": [
    "### Std of loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "surrounded-whale",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clip gradient: True, std of loss: 87.75686626245346\n",
      "clip gradient: False, std of loss: 128.1899696474328\n"
     ]
    }
   ],
   "source": [
    "for opt in [True, False]:\n",
    "    log_dir = f'log/clip-grad-{opt}'\n",
    "    logger = Logger(log_dir)\n",
    "    data = logger.load()\n",
    "    res = std(data, 'loss')\n",
    "    print(f\"clip gradient: {opt}, std of loss: {res}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-thesis",
   "metadata": {},
   "source": [
    "## Length of each trajectory\n",
    "One trajectory (from parallel env) is used for one step policy and value iteration. Intuitively, longer trajectory helps the agent to learn the long term benefit of its action. \n",
    "\n",
    "* Longer trajectory makes training more stable\n",
    "* Longer trajectory makes the agent learn faster\n",
    "* Effect of a bad action can only surface after a few steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "common-potato",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = {\n",
    "    \"n_iters\": 1000,\n",
    "    \"nenvs\": 16, # to be changed\n",
    "    \"ckpt_interval\": 10,\n",
    "    #\"nsteps\" : 30, # length of trajectory\n",
    "    \"entropy_coef\" : 0.01, #1e-4, \n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"p_coef\": 1.0,\n",
    "    \"v_coef\":1.0,\n",
    "    \"gamma\": 0.99,\n",
    "    \"device\": \"cpu\",\n",
    "    \"clip_grad_norm\": True,\n",
    "    #\"ckpt_dir\": 'ckpt/debug', # to be changed \n",
    "    #\"log_dir\": 'log/debug' # to be changed\n",
    "}\n",
    "\n",
    "for nsteps in range(5, 31):\n",
    "    h[\"nsteps\"] = nsteps\n",
    "    h['ckpt_dir'] = f'ckpt/nsteps-{nsteps}'\n",
    "    h['log_dir'] = f'log/nsteps-{nsteps}'\n",
    "    #a2c(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "greater-tradition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trajectory length: 5 | std of ma reward: 6.206312381559107\n",
      "trajectory length: 6 | std of ma reward: 6.7941367262294765\n",
      "trajectory length: 7 | std of ma reward: 11.88877426088386\n",
      "trajectory length: 8 | std of ma reward: 12.65766606890771\n",
      "trajectory length: 9 | std of ma reward: 12.049337735996671\n",
      "trajectory length: 10 | std of ma reward: 14.811946064503886\n",
      "trajectory length: 11 | std of ma reward: 16.305581726869427\n",
      "trajectory length: 12 | std of ma reward: 20.12604333236117\n",
      "trajectory length: 13 | std of ma reward: 15.68693050207474\n",
      "trajectory length: 14 | std of ma reward: 22.111985664898228\n",
      "trajectory length: 15 | std of ma reward: 19.667176656598706\n",
      "trajectory length: 16 | std of ma reward: 22.924070967874297\n",
      "trajectory length: 17 | std of ma reward: 24.94240039339594\n",
      "trajectory length: 18 | std of ma reward: 25.832477444081537\n",
      "trajectory length: 19 | std of ma reward: 27.05817743367623\n",
      "trajectory length: 20 | std of ma reward: 26.653663867526497\n",
      "trajectory length: 21 | std of ma reward: 24.835131565403454\n",
      "trajectory length: 22 | std of ma reward: 27.88279425568999\n",
      "trajectory length: 23 | std of ma reward: 28.367172789208418\n",
      "trajectory length: 24 | std of ma reward: 24.42656539757199\n",
      "trajectory length: 25 | std of ma reward: 30.38063516748396\n",
      "trajectory length: 26 | std of ma reward: 31.277589413527796\n",
      "trajectory length: 27 | std of ma reward: 31.685631787430765\n",
      "trajectory length: 28 | std of ma reward: 27.618030052540696\n",
      "trajectory length: 29 | std of ma reward: 28.184460387056014\n",
      "trajectory length: 30 | std of ma reward: 28.69954193210446\n"
     ]
    }
   ],
   "source": [
    "# std of moving average reward\n",
    "\n",
    "for nsteps in range(5, 31):\n",
    "    log_dir = f'log/nsteps-{nsteps}'\n",
    "    logger = Logger(log_dir)\n",
    "    data = logger.load()\n",
    "    res = std(data, 'ma_rew')\n",
    "    print(f'trajectory length: {nsteps} | std of ma reward: {res}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-value",
   "metadata": {},
   "source": [
    "# Oops! Pitfall\n",
    "\n",
    "I guess I was implicitly thinking that higher reward and stability are positively correlated, because they are both desirable. \n",
    "\n",
    "Randomness\n",
    "* randomness of the env\n",
    "* randomess of action (sampled from prob)\n",
    "\n",
    "This means\n",
    "* Longer trajectory has \"more randomness\"\n",
    "* => longer trajectory incurs a higher variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "unexpected-encounter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trajectory length: 5 std of loss: 15.95312442503258\n",
      "trajectory length: 6 std of loss: 27.050928839895633\n",
      "trajectory length: 7 std of loss: 25.569796520339374\n",
      "trajectory length: 8 std of loss: 23.259475564346605\n",
      "trajectory length: 9 std of loss: 27.114428326483672\n",
      "trajectory length: 10 std of loss: 30.20247197956925\n",
      "trajectory length: 11 std of loss: 38.41508256160848\n",
      "trajectory length: 12 std of loss: 49.422973096028386\n",
      "trajectory length: 13 std of loss: 42.26107678619629\n",
      "trajectory length: 14 std of loss: 57.51039843141428\n",
      "trajectory length: 15 std of loss: 51.04221888132379\n",
      "trajectory length: 16 std of loss: 62.58077858350786\n",
      "trajectory length: 17 std of loss: 66.96665101645331\n",
      "trajectory length: 18 std of loss: 66.97736167957794\n",
      "trajectory length: 19 std of loss: 76.10777375961906\n",
      "trajectory length: 20 std of loss: 75.51169820482042\n",
      "trajectory length: 21 std of loss: 67.49711880268305\n",
      "trajectory length: 22 std of loss: 75.16493681600943\n",
      "trajectory length: 23 std of loss: 74.21857834956097\n",
      "trajectory length: 24 std of loss: 70.48070160844658\n",
      "trajectory length: 25 std of loss: 88.83350108945598\n",
      "trajectory length: 26 std of loss: 89.1579255679235\n",
      "trajectory length: 27 std of loss: 82.21110429785736\n",
      "trajectory length: 28 std of loss: 78.36282909169032\n",
      "trajectory length: 29 std of loss: 88.01356780466357\n",
      "trajectory length: 30 std of loss: 82.05633115208828\n"
     ]
    }
   ],
   "source": [
    "# std of loss\n",
    "\n",
    "for nsteps in range(5, 31):\n",
    "    log_dir = f'log/nsteps-{nsteps}'\n",
    "    logger = Logger(log_dir)\n",
    "    data = logger.load()\n",
    "    res = std(data, 'loss')\n",
    "    print(f'trajectory length: {nsteps}', f'std of loss: {res}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "dressed-increase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trajectory length: 5 std of adv: 1.2608268788123396\n",
      "trajectory length: 6 std of adv: 1.5251485703363248\n",
      "trajectory length: 7 std of adv: 1.4852788971378308\n",
      "trajectory length: 8 std of adv: 1.5448067042383975\n",
      "trajectory length: 9 std of adv: 1.8090801431709604\n",
      "trajectory length: 10 std of adv: 1.7357480356143165\n",
      "trajectory length: 11 std of adv: 2.0372059369785824\n",
      "trajectory length: 12 std of adv: 2.1565931216087257\n",
      "trajectory length: 13 std of adv: 2.2502100934415523\n",
      "trajectory length: 14 std of adv: 2.2822920096137005\n",
      "trajectory length: 15 std of adv: 2.357039214276123\n",
      "trajectory length: 16 std of adv: 2.5615997374533688\n",
      "trajectory length: 17 std of adv: 2.5606363888938106\n",
      "trajectory length: 18 std of adv: 2.6407756721965017\n",
      "trajectory length: 19 std of adv: 2.8086783766190107\n",
      "trajectory length: 20 std of adv: 2.871514971747077\n",
      "trajectory length: 21 std of adv: 2.795736732860649\n",
      "trajectory length: 22 std of adv: 2.8621769379317827\n",
      "trajectory length: 23 std of adv: 2.8838695771115392\n",
      "trajectory length: 24 std of adv: 3.080788221664063\n",
      "trajectory length: 25 std of adv: 3.0134984435572294\n",
      "trajectory length: 26 std of adv: 3.0386913909370876\n",
      "trajectory length: 27 std of adv: 3.153793838496613\n",
      "trajectory length: 28 std of adv: 3.094768528985399\n",
      "trajectory length: 29 std of adv: 3.2609970622816493\n",
      "trajectory length: 30 std of adv: 3.276291629418475\n"
     ]
    }
   ],
   "source": [
    "# std of adv\n",
    "\n",
    "for nsteps in range(5, 31):\n",
    "    log_dir = f'log/nsteps-{nsteps}'\n",
    "    logger = Logger(log_dir)\n",
    "    data = logger.load()\n",
    "    res = std(data, 'adv')\n",
    "    print(f'trajectory length: {nsteps}', f'std of adv: {res}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "parental-think",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trajectory length: 5, last ma reward 32.723724365234375\n",
      "trajectory length: 6, last ma reward 30.767850875854492\n",
      "trajectory length: 7, last ma reward 70.91400146484375\n",
      "trajectory length: 8, last ma reward 67.55679321289062\n",
      "trajectory length: 9, last ma reward 76.67071533203125\n",
      "trajectory length: 10, last ma reward 88.3529281616211\n",
      "trajectory length: 11, last ma reward 56.044403076171875\n",
      "trajectory length: 12, last ma reward 85.94941711425781\n",
      "trajectory length: 13, last ma reward 80.94585418701172\n",
      "trajectory length: 14, last ma reward 96.23628234863281\n",
      "trajectory length: 15, last ma reward 84.31098175048828\n",
      "trajectory length: 16, last ma reward 115.69566345214844\n",
      "trajectory length: 17, last ma reward 115.28688049316406\n",
      "trajectory length: 18, last ma reward 122.9968490600586\n",
      "trajectory length: 19, last ma reward 104.73279571533203\n",
      "trajectory length: 20, last ma reward 104.3149642944336\n",
      "trajectory length: 21, last ma reward 123.93647766113281\n",
      "trajectory length: 22, last ma reward 124.37062072753906\n",
      "trajectory length: 23, last ma reward 109.558837890625\n",
      "trajectory length: 24, last ma reward 111.16752624511719\n",
      "trajectory length: 25, last ma reward 120.23048400878906\n",
      "trajectory length: 26, last ma reward 117.12339782714844\n",
      "trajectory length: 27, last ma reward 113.77822875976562\n",
      "trajectory length: 28, last ma reward 92.52961730957031\n",
      "trajectory length: 29, last ma reward 115.61091613769531\n",
      "trajectory length: 30, last ma reward 86.54060363769531\n"
     ]
    }
   ],
   "source": [
    "# moving average of the reward\n",
    "# at the end of the training\n",
    "\n",
    "for nsteps in range(5, 31):\n",
    "    log_dir = f'log/nsteps-{nsteps}'\n",
    "    logger = Logger(log_dir)\n",
    "    data = logger.load()\n",
    "    res = data['ma_rew'][-1][1]\n",
    "    print(f\"trajectory length: {nsteps}, last ma reward {res}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-endorsement",
   "metadata": {},
   "source": [
    "Our intuition seems to be right here. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-facing",
   "metadata": {},
   "source": [
    "## clip output of critic\n",
    "If the output of critic has too much variance, then it defies the purpose of using a critic for actor-critic flavored algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-afghanistan",
   "metadata": {},
   "source": [
    "## N && B: encourage exploration\n",
    "\n",
    "* policy gradient methods are already exploratory, actions are sampled from action distribution rather than greedy\n",
    "* want to avoid agent converge (over-confident) in the early stage\n",
    "* want to make entropy of action distribution higher in the early stage \n",
    "* different random seed for each env\n",
    "* start the env with some random actions\n",
    "\n",
    "\n",
    "### Entropy\n",
    "Entropy is baked into the loss\n",
    "\n",
    "Let $\\tau$ be a sample trajectory, $\\alpha$ entropy coeff (temperature)\n",
    "\n",
    "$\\sum $ means $\\sum_{s_i, a_i \\in \\tau}$\n",
    "\n",
    "$$\n",
    "Loss(\\tau) = - \\frac{1}{N} \\sum\\log \\pi(a_i|s_i) A(a_i, s_i)  + \\frac{1}{N}\\sum|Q(a_i, s_i) - V(s_i) |^2 + \\frac{1}{N}\\alpha \\sum \\entropy(\\pi(\\cdot | s_i))\n",
    "$$\n",
    "\n",
    "Better exploration correlated with better rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "stretch-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "h = {\n",
    "    \"n_iters\": 1000,\n",
    "    \"nenvs\": 16, # to be changed\n",
    "    \"ckpt_interval\": 10,\n",
    "    \"nsteps\" : 30, # length of trajectory\n",
    "    #\"entropy_coef\" : 0, \n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"p_coef\": 1.0,\n",
    "    \"v_coef\":1.0,\n",
    "    \"gamma\": 0.99,\n",
    "    \"device\": \"cpu\",\n",
    "    \"clip_grad_norm\": True,\n",
    "    #\"ckpt_dir\": 'ckpt/debug', # to be changed \n",
    "    #\"log_dir\": 'log/debug' # to be changed\n",
    "}\n",
    "\n",
    "\n",
    "for entropy_coef in [0, 0.01, 0.1]:\n",
    "    ckpt_dir =f'ckpt/entropy-coef-{entropy_coef}'\n",
    "    log_dir = f'log/entropy-coef-{entropy_coef}'\n",
    "    h['entropy_coef'] = entropy_coef\n",
    "    h['ckpt_dir'] = ckpt_dir\n",
    "    h['log_dir'] = log_dir\n",
    "    a2c(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-river",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "sacred-refund",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy coef: 0, average reward 72.5942310142517\n",
      "entropy coef: 0.1, average reward 64.95612062072755\n",
      "entropy coef: 0.01, average reward 71.91440085220337\n"
     ]
    }
   ],
   "source": [
    "# compare the rewards of experiements with\n",
    "# different entropy coefficient\n",
    "for ec in [0, 0.1, 0.01]:\n",
    "    log_dir = f'log/entropy-coef-{ec}'\n",
    "    data = Logger(log_dir).load()\n",
    "    avg_reward = [x[1] for x in data['ma_rew']]\n",
    "    print(f\"entropy coef: {ec}, average reward {sum(avg_reward) / len(avg_reward)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "external-lender",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy coef: 0, average reward 125.38384674072266\n",
      "entropy coef: 0.1, average reward 113.5899333190918\n",
      "entropy coef: 0.01, average reward 123.38497283935547\n"
     ]
    }
   ],
   "source": [
    "# average reward at latter stage of training\n",
    "for ec in [0, 0.1, 0.01]:\n",
    "    log_dir = f'log/entropy-coef-{ec}'\n",
    "    data = Logger(log_dir).load()\n",
    "    avg_reward = [x[1] for x in data['ma_rew']][-100:]\n",
    "    print(f\"entropy coef: {ec}, average reward {sum(avg_reward) / len(avg_reward)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "broad-spiritual",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<algo.Agent at 0x7f0c3446e2b0>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linearly decay entropy coefficient\n",
    "\n",
    "\n",
    "h = {\n",
    "    \"n_iters\": 1000,\n",
    "    \"nenvs\": 16, # to be changed\n",
    "    \"ckpt_interval\": 10,\n",
    "    \"nsteps\" : 30, # length of trajectory\n",
    "    \"entropy_coef\" : 0.1, \n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"p_coef\": 1.0,\n",
    "    \"v_coef\":1.0,\n",
    "    \"gamma\": 0.99,\n",
    "    \"device\": \"cpu\",\n",
    "    \"clip_grad_norm\": True,\n",
    "    #\"ckpt_dir\": 'ckpt/debug', # to be changed \n",
    "    #\"log_dir\": 'log/debug' # to be changed\n",
    "}\n",
    "\n",
    "def entropy_decay_fn(curr_coef, n_iter):\n",
    "    return -(0.1/1000.0) * n_iter + 0.1\n",
    "\n",
    "ckpt_dir = 'ckpt/entropy-decay'\n",
    "log_dir = 'ckpt/entropy-decay'\n",
    "h['ckpt_dir'] = ckpt_dir \n",
    "h['log_dir'] = log_dir\n",
    "\n",
    "decay_fn = {\"entropy_coef\": entropy_decay_fn}\n",
    "\n",
    "a2c(h, decay_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "fitting-emission",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with entropy decay, average reward 63.838873136520384\n"
     ]
    }
   ],
   "source": [
    "og_dir = 'ckpt/entropy-decay'\n",
    "data = Logger(log_dir).load()\n",
    "avg_reward = [x[1] for x in data['ma_rew']]\n",
    "print(f\"with entropy decay, average reward {sum(avg_reward) / len(avg_reward)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "olive-divide",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with entropy decay, average reward 123.38497283935547\n"
     ]
    }
   ],
   "source": [
    "# later stage of training\n",
    "og_dir = 'ckpt/entropy-decay'\n",
    "data = Logger(log_dir).load()\n",
    "avg_reward = [x[1] for x in data['ma_rew']][-100:]\n",
    "print(f\"with entropy decay, average reward {sum(avg_reward) / len(avg_reward)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-basket",
   "metadata": {},
   "source": [
    "## N && B: check if critic is learning anything by viewing the estimate of initial state\n",
    "\n",
    "For Cartpole, the initial state should have have high value (for a trained agent), so we can evalutate critic by \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "ruled-armor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<algo.Agent at 0x7f0c34635780>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = {\n",
    "    \"n_iters\": 1000,\n",
    "    \"nenvs\": 16, # to be changed\n",
    "    \"ckpt_interval\": 100,\n",
    "    \"nsteps\" : 30, # length of trajectory\n",
    "    \"entropy_coef\" : 0., \n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"p_coef\": 1.0,\n",
    "    \"v_coef\":1.0,\n",
    "    \"gamma\": 0.99,\n",
    "    \"device\": \"cpu\",\n",
    "    \"clip_grad_norm\": True,\n",
    "    \"evalute_critic\": True\n",
    "    #\"ckpt_dir\": 'ckpt/debug', # to be changed \n",
    "    #\"log_dir\": 'log/debug' # to be changed\n",
    "}\n",
    "\n",
    "ckpt_dir = 'ckpt/evaluate-critic'\n",
    "log_dir = 'log/evaluate-critic'\n",
    "h['ckpt_dir'] = ckpt_dir \n",
    "h['log_dir'] = log_dir\n",
    "\n",
    "a2c(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "comparable-discussion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'estimate of initial state')"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "log_dir = 'log/evaluate-critic'\n",
    "init_state_estimate = Logger(log_dir).load()['init_state_estimate']\n",
    "init_state_estimate = [x[1] for x in init_state_estimate]\n",
    "\n",
    "plt.plot(init_state_estimate)\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel('estimate of initial state')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-reality",
   "metadata": {},
   "source": [
    "## N && B: policy net and value net initialization\n",
    "\n",
    "orthogonal initiazation\n",
    "\n",
    "[provable benefit of orthogonal init](https://openreview.net/pdf?id=rkgqN1SYvr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-officer",
   "metadata": {},
   "source": [
    "## N && B: separate backbone or 1 backbone\n",
    "Just try it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "established-quantity",
   "metadata": {},
   "source": [
    "## N && B: when something is wrong, look at what the policy is doing\n",
    "The agent seems to not be able to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "rocky-course",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "import gym\n",
    "\n",
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.axis('off')\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "coral-worker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVQAAADnCAYAAABBu67aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAADqUlEQVR4nO3c200CURRA0RlDE9ZhG9ahNWkdlqF1WMb1zxgfM0PYSIC1EhLCHcJ87ZzACfMYYwLgcDenvgGASyGoABFBBYgIKkBEUAEiu5VzKwAAP82/vWhCBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKsAe3p4f/zwTVICIoAJEBBUgIqgAEUEFiAgqQERQASKCCrDR0g7qNAkqQEZQASKCChARVICIoAJEBBUgIqgAEUEF2GBtB3WaBBUgI6gAEUEFiAgqQERQASKCChARVIAVW1ampklQATKCClyleZ43P7YSVIDI7tQ3AHAOXt4fPp/f/XGNCRVgxdeYLhFUgIigAkQEFWDB69PDdH/7vOnaeYyxdL54CHCu9lmH+m6M8eubF3/lP+QDAa7NYlBXpleAs3WMgdF3qAARQQWICCpARFABIoIKEBFUgIigAkT8fR9wlY6xZ29CBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAyG7lfP6XuwC4ACZUgIigAkQEFSAiqAARQQWICCpA5AMziiEs/VT90wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 194 timesteps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from algo import Agent\n",
    "from model import MLPSingleArch\n",
    "\n",
    "h = {\n",
    "    \"n_iters\": 1000,\n",
    "    \"nenvs\": 16, # to be changed\n",
    "    \"ckpt_interval\": 100,\n",
    "    \"nsteps\" : 10, # length of trajectory\n",
    "    \"entropy_coef\" : 0., \n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"p_coef\": 1.0,\n",
    "    \"v_coef\":1.0,\n",
    "    \"gamma\": 0.99,\n",
    "    \"device\": \"cpu\",\n",
    "    \"clip_grad_norm\": True,\n",
    "    \"evalute_critic\": True\n",
    "    #\"ckpt_dir\": 'ckpt/debug', # to be changed \n",
    "    #\"log_dir\": 'log/debug' # to be changed\n",
    "}\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "observation = env.reset()\n",
    "\n",
    "agent = Agent(\n",
    "    model = MLPSingleArch(\n",
    "        input_dim = env.observation_space.shape[0],\n",
    "        n_actions=env.action_space.n\n",
    "    ), h=h)\n",
    "\n",
    "agent.resume('ckpt/nsteps-7/ckpt-999.pt')\n",
    "    \n",
    "for t in range(1000):\n",
    "    action = agent.take_action(observation, greedy=True)\n",
    "    show_state(env, t)\n",
    "    observation, reward, done, info = env.step(action[0])\n",
    "    if done:\n",
    "        print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "        plt.clf()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "united-function",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVQAAADnCAYAAABBu67aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAADrUlEQVR4nO3cwU0CQQBA0V1DE/ZkHViT1mEZWodljDdjFHYhfETkvYTDMmSZ089kmGUeY0wAnO7u0hMA+C8EFSAiqAARQQWICCpAZLMy7ggAwE/zrjetUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoAEd4e37cOyaoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBTjQ0lNS0ySoABlBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBTjA2lNS0ySoABlBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoACsOOdQ/TYIKkBFUgIigAkQEFSAiqAARQQWICCpARFCBmzPP81GvQwkqQGRz6QkA/GWvT9vp5X37ef1w/7z3s1aoAAu+xnTX9VeCChARVICIoAIs+L5nurSHOo8xlu61OAhwjY45CrXLGGPnDRZ/5T/1SwFuyWJQV1avAFfpXItFe6gAEUEFiAgqQERQASKCChARVICIoAJE/H0fcHPOdcbeChUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKblfH5V2YB8A9YoQJEBBUgIqgAEUEFiAgqQERQASIfjwsk+xS0LM0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 229 timesteps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0').unwrapped\n",
    "observation = env.reset()\n",
    "\n",
    "agent = Agent(\n",
    "    model = MLPSingleArch(\n",
    "        input_dim = env.observation_space.shape[0],\n",
    "        n_actions=env.action_space.n\n",
    "    ), h=h)\n",
    "\n",
    "agent.resume('ckpt/nsteps-29/ckpt-999.pt')\n",
    "    \n",
    "for t in range(1000):\n",
    "    action = agent.take_action(observation, greedy=True)\n",
    "    show_state(env, t)\n",
    "    observation, reward, done, info = env.step(action[0])\n",
    "    if done:\n",
    "        print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "        plt.clf()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "pending-dialogue",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVQAAADnCAYAAABBu67aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAADnElEQVR4nO3c0U3CYBhA0dawhHO4hnPITp3DNZjDMX6ffFFoG72RAOckvPCV8D/dfCkN8xhjAuDvnq59AIB7IagAEUEFiAgqQERQASKHjblHAAB+ms+9aUMFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoALsdFqO02k5XpwLKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBdjhtBw3rxFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQUe1jzPu197CCpA5HDtAwDcivePt2mapunlwtyGCrDDV0zXCCpARFABIoIKsMPr87J5zTzGWJuvDgFu2d7Hob4bY5z94Oqv/L/9MoBHtBrUje0V4KbVS6N7qAARQQWICCpARFABIoIKEBFUgIigAkT8fR/wsOpn7W2oABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQOWzM5385BcAdsKECRAQVICKoABFBBYgIKkBEUAEin1yAHZMM29kIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 1251 timesteps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0').unwrapped\n",
    "observation = env.reset()\n",
    "\n",
    "agent = Agent(\n",
    "    model = MLPSingleArch(\n",
    "        input_dim = env.observation_space.shape[0],\n",
    "        n_actions=env.action_space.n\n",
    "    ), h=h)\n",
    "\n",
    "agent.resume('ckpt/nsteps-10/ckpt-999.pt')\n",
    "    \n",
    "for t in range(1500):\n",
    "    action = agent.take_action(observation, greedy=True)\n",
    "    show_state(env, t)\n",
    "    observation, reward, done, info = env.step(action[0])\n",
    "    if done:\n",
    "        print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "        plt.clf()\n",
    "        break\n",
    "plt.clf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
