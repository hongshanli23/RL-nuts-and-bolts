{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-argument",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem is at conjugate gradient\n",
    "# make sure my torch conjugate gradient and the openai's numpy conjugate gradient algo are \n",
    "# correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "rotary-rider",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from cg import cg, _cg\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "A = np.random.rand(5, 5)\n",
    "b = np.random.rand(5)\n",
    "\n",
    "def linear_fn(x):\n",
    "    return A@x\n",
    "\n",
    "a = _cg(linear_fn, b, cg_iters=10)\n",
    "\n",
    "A = torch.from_numpy(A)\n",
    "b = torch.from_numpy(b)\n",
    "\n",
    "a_ = cg(linear_fn, b, cg_iters=10).numpy()\n",
    "\n",
    "print(np.allclose(a, a_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "married-effects",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum of gradient tensor(1.9902)\n",
      "      iter residual norm  soln norm\n",
      "         0       12.5          0\n",
      "         1   1.16e+04         46\n",
      "         2       1.47       1.51\n",
      "         3       53.5       4.72\n",
      "         4      0.633       1.66\n",
      "         5       15.7       3.39\n",
      "         6      0.277       1.73\n",
      "         7       3.51       2.26\n",
      "         8     0.0947       1.76\n",
      "         9      0.606       1.86\n",
      "        10     0.0438       1.77\n"
     ]
    }
   ],
   "source": [
    "# check if my implementation of FVP can correctly compute the \n",
    "# Av\n",
    "import models\n",
    "import cg\n",
    "import imp\n",
    "\n",
    "imp.reload(models)\n",
    "imp.reload(cg)\n",
    "import utils as U\n",
    "from models import MLPTest\n",
    "from cg import cg\n",
    "\n",
    "\n",
    "input_shape=4\n",
    "model = MLPTest(input_shape)\n",
    "cg_damping = 1e-2\n",
    "\n",
    "ob = torch.rand(12, input_shape)\n",
    "kl = model(ob).mean()\n",
    "\n",
    "\n",
    "def compute_fvp(flat_tangent, ob):\n",
    "    dkl_params = U.flatten(\n",
    "        torch.autograd.grad(kl, model.parameters(), \n",
    "                            retain_graph=True,\n",
    "                           create_graph=True))\n",
    "    \n",
    "    z = torch.dot(dkl_params, flat_tangent)\n",
    "    return U.flatten(torch.autograd.grad(z, model.parameters(), \n",
    "                                         retain_graph=True))\n",
    "\n",
    "def fisher_vector_product(p):\n",
    "    return compute_fvp(p, ob)\n",
    "\n",
    "g = U.flatten(torch.autograd.grad(kl, model.parameters(), retain_graph=True))\n",
    "print('sum of gradient', g.sum())\n",
    "\n",
    "x=cg(fisher_vector_product, g, cg_iters=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "rolled-hamilton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y,  *_ = np.arange(10)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "casual-bhutan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "amber-input",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0a8ae4297e55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m# Generate a positve definite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m# symmetric matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1e9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def mycg(f_Ax, b, cg_iters=10, verbose=False, residual_tol=1e-10):\n",
    "    x = torch.zeros_like(b, dtype=torch.float32)\n",
    "    \n",
    "    # residual and A-conjugate basis\n",
    "    r, p = b.clone(), b.clone()\n",
    "    rr = torch.dot(r, r)\n",
    "    \n",
    "    fmtstr =  \"%10i %10.3g %10.3g\"\n",
    "    titlestr =  \"%10s %10s %10s\"\n",
    "    if verbose: print(titlestr % (\"iter\", \"residual norm\", \"soln norm\"))\n",
    "        \n",
    "    for i in range(cg_iters):\n",
    "        if verbose: print(fmtstr % (i, rr, np.linalg.norm(x.numpy())))\n",
    "        Ap = f_Ax(p)\n",
    "        assert p.shape == Ap.shape\n",
    "        alpha = rr / torch.dot(p, Ap)\n",
    "        x = x + alpha * p\n",
    "        r = r - alpha * Ap\n",
    "        rr_new = torch.dot(r, r)\n",
    "        beta = - rr_new / rr\n",
    "        p = r - beta * p\n",
    "        rr = rr_new\n",
    "        if rr < residual_tol:\n",
    "            print(\"residual tolerance achieved, good enough for now\")\n",
    "            break\n",
    "    \n",
    "    if verbose: print(fmtstr % (i+1, rr, np.linalg.norm(x.numpy())))\n",
    "    return x\n",
    "        \n",
    "def mycg_numpy(f_Ax, b, cg_iters=10, verbose=False, residual_tol=1e-10):\n",
    "    x = np.zeros_like(b, dtype=np.float32)\n",
    "    \n",
    "    # residual and A-conjugate basis\n",
    "    r, p = b.copy(), b.copy()\n",
    "    rr = np.dot(r, r)\n",
    "    \n",
    "    fmtstr =  \"%10i %10.3g %10.3g\"\n",
    "    titlestr =  \"%10s %10s %10s\"\n",
    "    if verbose: print(titlestr % (\"iter\", \"residual norm\", \"soln norm\"))\n",
    "        \n",
    "    for i in range(cg_iters):\n",
    "        if verbose: print(fmtstr % (i, rr, np.linalg.norm(x)))\n",
    "        Ap = f_Ax(p)\n",
    "        assert p.shape == Ap.shape\n",
    "        alpha = rr / np.dot(p, Ap)\n",
    "        x = x + alpha * p\n",
    "        r = r - alpha * Ap\n",
    "        rr_new = np.dot(r, r)\n",
    "        beta = - rr_new / rr\n",
    "        p = r - beta * p\n",
    "        rr = rr_new\n",
    "        if rr < residual_tol:\n",
    "            print(\"residual tolerance achieved, good enough for now\")\n",
    "            break\n",
    "    \n",
    "    if verbose: print(fmtstr % (i+1, rr, np.linalg.norm(x)))\n",
    "    return x\n",
    "        \n",
    "\n",
    "\n",
    "n = 36\n",
    "\n",
    "# Generate a positve definite\n",
    "# symmetric matrix\n",
    "A = np.random.rand(n, n)\n",
    "A = A@A.T/1e9\n",
    "\n",
    "print('sum of A', np.sum(A))\n",
    "\n",
    "g = np.random.rand(n)\n",
    "\n",
    "def linear_fn(p):\n",
    "    return np.dot(A, p)\n",
    "\n",
    "\n",
    "\n",
    "x = mycg_numpy(linear_fn, g, cg_iters=15, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-hopkins",
   "metadata": {},
   "source": [
    "I roughly know where the problem is, look at the surrgain from my implementation\n",
    "It is on the scale of 10^2\n",
    "```\n",
    "********** Iteration 1 ************\n",
    "sampling\n",
    "done in 0.395 seconds\n",
    "XXXXX atarg shape torch.Size([1024])\n",
    "mean atarg tensor(-2.2631e-07)\n",
    "computegrad\n",
    "done in 0.003 seconds\n",
    "cg\n",
    "sum of g tensor(8.5350)\n",
    "      iter residual norm  soln norm\n",
    "         0   1.34e+03          0\n",
    "         1        204       13.2\n",
    "         2       35.5       16.5\n",
    "         3       10.9       41.2\n",
    "         4       4.07       44.9\n",
    "         5     0.0867       73.3\n",
    "         6       1.99       75.3\n",
    "         7     0.0149       75.9\n",
    "         8      0.031       75.9\n",
    "         9   3.12e-05       76.9\n",
    "        10   0.000249       76.9\n",
    "done in 0.030 seconds\n",
    "Expected: 1.301 Actual: 0.705\n",
    "stepsize ok\n",
    "vf\n",
    "done in 0.002 seconds\n",
    "---------------------------------------\n",
    "| entloss         | tensor(0.)        |\n",
    "| entropy         | tensor(1.4030)    |\n",
    "| EpisodesSoFar   | 0                 |\n",
    "| EpLenMean       | 200               |\n",
    "| EpRewMean       | -1.27e+03         |\n",
    "| EpThisIter      | 5                 |\n",
    "| ev_tdlam_before | 2.38e-07          |\n",
    "| meankl          | tensor(0.0010)    |\n",
    "| optimgain       | tensor(-356.8666) |\n",
    "| surrgain        | tensor(-356.8666) |\n",
    "| TimeElapsed     | 0.97              |\n",
    "| TimestepsSoFar  | 2000              |\n",
    "---------------------------------------\n",
    "```\n",
    "\n",
    "OpenAi's implementation is \n",
    "\n",
    "```----------------------------------\n",
    "| entloss         | 0.0          |\n",
    "| entropy         | 1.4265722    |\n",
    "| EpisodesSoFar   | 5            |\n",
    "| EpLenMean       | 200          |\n",
    "| EpRewMean       | -1317.9513   |\n",
    "| EpThisIter      | 5            |\n",
    "| ev_tdlam_before | -0.000287    |\n",
    "| meankl          | 0.0007376976 |\n",
    "| optimgain       | 0.0027435264 |\n",
    "| surrgain        | 0.0027435264 |\n",
    "| TimeElapsed     | 2.19         |\n",
    "| TimestepsSoFar  | 1000         |\n",
    "----------------------------------\n",
    "\n",
    "```\n",
    "\n",
    "Their surrgain is confined in much smaller scale. I need to figure out why that's the case. \n",
    "When I removed advtange from the surrgain computation, my implmentaiton and openai agree. \n",
    "\n",
    "Our atarg (advantage) is on the same scale after normalize."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
